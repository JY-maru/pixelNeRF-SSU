import os
import sys
import glob
import argparse
import torch
import torch.nn.functional as F
import numpy as np
import imageio
from tqdm import tqdm
from PIL import Image
from torchvision import transforms

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(os.getcwd())

from config.config import Config
from models.pixelnerf import PixelNeRF
from utils.geometry import CameraUtils

# ==============================================================================
# 1. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (Inference.py ë¡œì§ ì¬ì‚¬ìš© + TTOìš© ë³€í˜•)
# ==============================================================================
ROTATION_MATRIX = torch.tensor([
    [ 0,  1,  0,  0],   # New X = Old Y
    [-1,  0,  0,  0],   # New Y = -Old X
    [ 0,  0,  1,  0],   # Z ìœ ì§€
    [ 0,  0,  0,  1]
], dtype=torch.float32)

def parse_pose_file(path):
    with open(path, 'r') as f:
        values = [float(x) for x in f.read().split()]
    return torch.tensor(values, dtype=torch.float32).reshape(4, 4)

def load_single_instance_data(folder_path, target_size=(128, 128), device='cuda'):
    """
    íŠ¹ì • í´ë”(Instance)ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ TTO í•™ìŠµìš© í…ì„œë¡œ ë°˜í™˜
    """
    rgb_dir = os.path.join(folder_path, 'rgb')
    pose_dir = os.path.join(folder_path, 'pose')
    
    img_paths = sorted(glob.glob(os.path.join(rgb_dir, "*.png")) + glob.glob(os.path.join(rgb_dir, "*.jpg")))
    pose_paths = sorted(glob.glob(os.path.join(pose_dir, "*.txt")))
    
    if not img_paths: return None

    # Transform: TTOì‹œì—ëŠ” Augmentation ì—†ì´ ë¦¬ì‚¬ì´ì¦ˆë§Œ
    transform = transforms.Compose([
        transforms.Resize(target_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    images, poses = [], []
    
    for i in range(len(img_paths)):
        # ì´ë¯¸ì§€ ë¡œë“œ
        img = Image.open(img_paths[i]).convert("RGB")
        images.append(transform(img))
        
        # í¬ì¦ˆ ë¡œë“œ ë° ì¢Œí‘œê³„ ë³€í™˜
        pose = parse_pose_file(pose_paths[i])
        pose = torch.matmul(ROTATION_MATRIX, pose)
        poses.append(pose)
    
    # Intrinsics (ShapeNet ê¸°ë³¸ê°’ ê°€ì • ë˜ëŠ” íŒŒì¼ ë¡œë“œ)
    # ì—¬ê¸°ì„œëŠ” ê°„ì†Œí™”ë¥¼ ìœ„í•´ ê¸°ë³¸ê°’ ì‚¬ìš© (inference.py ë¡œì§ ì°¸ê³ )
    focal = 0.5 * target_size[0] / np.tan(0.5 * np.deg2rad(50)) # FOV 50 assumption
    intrinsic = torch.tensor([
        [focal, 0, target_size[1]/2],
        [0, focal, target_size[0]/2],
        [0, 0, 1]
    ], dtype=torch.float32)
    
    intrinsics = intrinsic.unsqueeze(0).repeat(len(images), 1, 1) # (N, 3, 3)

    return {
        'images': torch.stack(images).to(device),       # (N, 3, H, W)
        'poses': torch.stack(poses).to(device),         # (N, 4, 4)
        'intrinsics': intrinsics.to(device)             # (N, 3, 3)
    }

# ==============================================================================
# 2. TTO ë° ë Œë”ë§ í´ë˜ìŠ¤
# ==============================================================================
class TTOHandler:
    def __init__(self, config, checkpoint_path):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.checkpoint_path = checkpoint_path
        self.base_model_state = None
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = PixelNeRF(
            encoder_type=config.model.encoder_type,
            encoder_pretrained=False, 
            feature_dim=config.model.feature_dim,
            d_hidden=config.model.d_hidden,
            n_blocks=config.model.n_blocks,
            combine_type=config.model.combine_type,
            n_coarse=config.model.n_coarse,
            n_fine=config.model.n_fine,
            white_bkgd=config.model.white_bkgd,
            use_pe=config.model.use_pe,
            pe_freq_pos=config.model.pe_freq_pos,
            pe_freq_dir=config.model.pe_freq_dir
        ).to(self.device)
        
        # ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë° ì €ì¥ (ë§¤ ì¸ìŠ¤í„´ìŠ¤ë§ˆë‹¤ ë¦¬ì…‹í•˜ê¸° ìœ„í•´)
        self._load_checkpoint(checkpoint_path)
        self.base_model_state = {k: v.clone() for k, v in self.model.state_dict().items()}
        print(f"âœ… Base model loaded from {checkpoint_path}")

    def _load_checkpoint(self, path):
        checkpoint = torch.load(path, map_location=self.device)
        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
        self.model.load_state_dict(state_dict)

    def reset_model(self):
        """ë‹¤ìŒ ìë™ì°¨ë¥¼ ìœ„í•´ ëª¨ë¸ì„ ì›ë³¸ ìƒíƒœë¡œ ë˜ëŒë¦¼"""
        self.model.load_state_dict(self.base_model_state)

    def optimize_instance(self, src_data, steps=500, lr=1e-5):
        """
        [í•µì‹¬] ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´ ëª¨ë¸ì„ íŒŒì¸íŠœë‹ (TTO)
        """
        self.model.train()
        
        # ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ (ë˜ëŠ” ì¸ì½”ë” ì œì™¸ ê°€ëŠ¥)
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        
        images = src_data['images']      # (N, 3, H, W)
        poses = src_data['poses']        # (N, 4, 4)
        intrinsics = src_data['intrinsics'] # (N, 3, 3)
        
        N, _, H, W = images.shape
        batch_rays = 1024 # ë©”ëª¨ë¦¬ ì•ˆì „ì„ ìœ„í•´ ì ë‹¹íˆ ì„¤ì •
        
        pbar = tqdm(range(steps), desc="âš¡ TTO Optimizing", leave=False)
        
        for _ in pbar:
            # 1. Random View & Pixel Sampling
            img_idx = np.random.randint(0, N)
            
            # í”½ì…€ ì¢Œí‘œ ëœë¤ ìƒì„±
            coords = torch.stack(torch.meshgrid(
                torch.arange(H, device=self.device),
                torch.arange(W, device=self.device)
            ), -1).reshape(-1, 2)
            
            select_inds = np.random.choice(coords.shape[0], size=[batch_rays], replace=False)
            select_coords = coords[select_inds] # (B, 2) -> (y, x) ìˆœì„œ ì£¼ì˜
            
            # Ground Truth Pixel Value ê°€ì ¸ì˜¤ê¸°
            # grid ì¢Œí‘œëŠ” (y, x) ìˆœì„œì´ë¯€ë¡œ indexing ì£¼ì˜
            target_rgb = images[img_idx, :, select_coords[:, 0], select_coords[:, 1]].T # (B, 3)
            
            # 2. Ray Generation (CameraUtils ì‚¬ìš©)
            # select_coordsëŠ” (y, x) -> (row, col)
            # get_rays_at_coords ê°™ì€ í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ ìˆ˜ë™ ê³„ì‚° ë˜ëŠ” get_rays í›„ ì¸ë±ì‹±
            # ì—¬ê¸°ì„œëŠ” íš¨ìœ¨ì„ ìœ„í•´ ì „ì²´ Ray ìƒì„± í›„ ì¸ë±ì‹± (H, Wê°€ í¬ì§€ ì•Šìœ¼ë¯€ë¡œ ê°€ëŠ¥)
            
            rays_o, rays_d = CameraUtils.get_rays(H, W, intrinsics[img_idx], poses[img_idx])
            # rays: (H, W, 3) -> reshape -> (H*W, 3)
            rays_o = rays_o.reshape(-1, 3)[select_inds] # (B, 3)
            rays_d = rays_d.reshape(-1, 3)[select_inds]
            
            # ì°¨ì› ì¶”ê°€ (Batch size 1 ê°„ì£¼)
            rays_o = rays_o.unsqueeze(0) # (1, B, 3)
            rays_d = rays_d.unsqueeze(0)
            
            # 3. Model Forward
            # PixelNeRFëŠ” conditioningì„ ìœ„í•´ src infoë¥¼ ë°›ìŒ
            # ì—¬ê¸°ì„œ srcëŠ” 'ìê¸° ìì‹ 'ì´ ë¨ (Few-shot learning)
            out = self.model(
                images.unsqueeze(0), 
                intrinsics.unsqueeze(0), 
                poses.unsqueeze(0),
                rays_o, 
                rays_d,
                z_near=self.config.data.z_near,
                z_far=self.config.data.z_far
            )
            
            rgb_pred = out['fine']['rgb_map'] # (1, B, 3)
            
            # 4. Loss & Backward
            loss = F.mse_loss(rgb_pred.squeeze(0), target_rgb)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            pbar.set_description(f"Loss: {loss.item():.4f}")

    def render_orbit(self, src_data, output_path, render_size=512):
        """í•™ìŠµëœ ëª¨ë¸ë¡œ ê³ í•´ìƒë„ ì˜¤ë¥´ë¹— ì˜ìƒ ë Œë”ë§"""
        self.model.eval()
        
        # ë Œë”ë§ìš© Intrinsic (í•´ìƒë„ ë³€ê²½ ë°˜ì˜)
        H_src, W_src = src_data['images'].shape[-2:]
        scale = render_size / H_src
        
        # Source 0ë²ˆ ë·°ì˜ Intrinsic ê°€ì ¸ì™€ì„œ ìŠ¤ì¼€ì¼ë§
        tgt_intrinsic = src_data['intrinsics'][0].clone()
        tgt_intrinsic[:2] *= scale
        
        # Orbit Pose ìƒì„±
        center = np.array([0., 0., 0.])
        radius = 1.5 # ì ì ˆí•œ ê±°ë¦¬ ì„¤ì •
        poses = self._get_orbit_poses(40, radius, center, elevation=30)
        
        frames = []
        chunk_size = 1024 # ë Œë”ë§ ì‹œ OOM ë°©ì§€
        
        print(f"ğŸ¥ Rendering video ({render_size}x{render_size})...")
        with torch.no_grad():
            for pose in tqdm(poses, desc="Rendering", leave=False):
                rays_o, rays_d = CameraUtils.get_rays(render_size, render_size, tgt_intrinsic, pose)
                rays_o = rays_o.reshape(-1, 3).unsqueeze(0)
                rays_d = rays_d.reshape(-1, 3).unsqueeze(0)
                
                rgb_chunks = []
                for i in range(0, rays_o.shape[1], chunk_size):
                    chunk_o = rays_o[:, i:i+chunk_size]
                    chunk_d = rays_d[:, i:i+chunk_size]
                    
                    out = self.model(
                        src_data['images'].unsqueeze(0), # Source Condition
                        src_data['intrinsics'].unsqueeze(0),
                        src_data['poses'].unsqueeze(0),
                        chunk_o, chunk_d,
                        self.config.data.z_near,
                        self.config.data.z_far
                    )
                    rgb_chunks.append(out['fine']['rgb_map'].cpu())
                
                img = torch.cat(rgb_chunks, dim=1).reshape(render_size, render_size, 3)
                img = torch.clamp(img, 0, 1).numpy()
                frames.append((img * 255).astype(np.uint8))
                
        imageio.mimsave(output_path, frames, fps=30)
        print(f"âœ¨ Saved to {output_path}")

    def _get_orbit_poses(self, num_frames, radius, center, elevation):
        # Orbit Pose ìƒì„± ë¡œì§ (inference.pyì™€ ë™ì¼í•œ ë°©ì‹ ì‚¬ìš© ê¶Œì¥)
        # ê°„ì†Œí™”ë¥¼ ìœ„í•´ ê°„ë‹¨í•œ ë¡œì§ êµ¬í˜„
        poses = []
        phi = np.deg2rad(90 - elevation)
        for i in range(num_frames):
            theta = 2 * np.pi * i / num_frames
            x = radius * np.sin(phi) * np.cos(theta)
            y = radius * np.sin(phi) * np.sin(theta)
            z = radius * np.cos(phi)
            
            cam_pos = np.array([x, y, z]) + center
            forward = center - cam_pos
            forward /= np.linalg.norm(forward)
            up = np.array([0, 0, 1])
            right = np.cross(forward, up)
            right /= np.linalg.norm(right)
            down = np.cross(forward, right)
            
            pose = np.eye(4)
            pose[:3, 0] = right
            pose[:3, 1] = down
            pose[:3, 2] = forward
            pose[:3, 3] = cam_pos
            
            pose_tensor = torch.from_numpy(pose).float()
            # í•™ìŠµ ë•Œ Rotation Matrix ì ìš©í–ˆë‹¤ë©´ ì—¬ê¸°ì„œë„ í•„ìš”
            pose_tensor = torch.matmul(ROTATION_MATRIX, pose_tensor)
            poses.append(pose_tensor)
            
        return torch.stack(poses).to(self.device)

# ==============================================================================
# 3. ë©”ì¸ ì‹¤í–‰ë¶€
# ==============================================================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='config/default_config.yaml')
    parser.add_argument('--test_data_dir', type=str, default='/content/pixNeRF_shapeNet_v2_data/cars_test')
    parser.add_argument('--tto_steps', type=int, default=500, help="Instanceë‹¹ í•™ìŠµ Step ìˆ˜")
    parser.add_argument('--tto_size', type=int, default=256, help="TTO í•™ìŠµ ì‹œ ì´ë¯¸ì§€ í•´ìƒë„ (ë†’ì„ìˆ˜ë¡ ë””í…Œì¼ ìœ ë¦¬)")
    parser.add_argument('--render_size', type=int, default=512, help="ìµœì¢… ë Œë”ë§ í•´ìƒë„")
    args = parser.parse_args()

    # 1. Config ë¡œë“œ
    config = Config.from_yaml(args.config)
    
    # 2. TTO í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (ëª¨ë¸ ë¡œë“œ)
    handler = TTOHandler(config, config.inference.checkpoint_path)
    
    # 3. Test ë°ì´í„° í´ë” ê²€ìƒ‰ (Config ë¬´ì‹œí•˜ê³  ì§ì ‘ ê²€ìƒ‰)
    if not os.path.exists(args.test_data_dir):
        print(f"âŒ Error: Test directory not found: {args.test_data_dir}")
        return

    instance_folders = sorted(glob.glob(os.path.join(args.test_data_dir, "*")))
    # í•˜ìœ„ í´ë”ì¸ì§€ í™•ì¸ (íŒŒì¼ ì œì™¸)
    instance_folders = [f for f in instance_folders if os.path.isdir(f)]
    
    print(f"Found {len(instance_folders)} test instances.")
    print(f"â„¹ï¸  TTO Settings: Steps={args.tto_steps}, TrainSize={args.tto_size}, RenderSize={args.render_size}")

    os.makedirs(config.inference.output_dir, exist_ok=True)

    # 4. ì „ì²´ ë£¨í”„ ì‹¤í–‰
    for idx, folder in enumerate(instance_folders):
        instance_name = os.path.basename(folder)
        print(f"\n[{idx+1}/{len(instance_folders)}] Processing: {instance_name}")
        
        # A. ëª¨ë¸ ë¦¬ì…‹ (ì´ì „ ìë™ì°¨ í•™ìŠµ ë‚´ìš© ì‚­ì œ)
        handler.reset_model()
        
        # B. ë°ì´í„° ë¡œë“œ (TTOìš© í•´ìƒë„ë¡œ ë¡œë“œ)
        # ì—¬ê¸°ì„œ tto_size(ì˜ˆ: 256)ë¥¼ ì¤˜ì„œ í•™ìŠµ ë•Œë³´ë‹¤ ë” í¬ê²Œ ë³´ê²Œ ë§Œë“¦
        data = load_single_instance_data(folder, target_size=(args.tto_size, args.tto_size))
        
        if data is None:
            print("   âš ï¸ No images found, skipping...")
            continue
            
        # C. Test-Time Optimization ìˆ˜í–‰
        handler.optimize_instance(data, steps=args.tto_steps, lr=1e-5)
        
        # D. ê²°ê³¼ ë Œë”ë§
        output_filename = os.path.join(config.inference.output_dir, f"TTO_{instance_name}.mp4")
        handler.render_orbit(data, output_filename, render_size=args.render_size)

if __name__ == "__main__":
    main()